————————————————————————————————————————————————————————————————————————————————————————————————————
【给定一个数据集，你如何决定使用哪一个算法】
	机器学习算法的选择取决于数据的类型。
	如果给定的一个数据集是线性的————线性回归
	如果数据是图像或者音频————神经网络
	如果该数据是非线性互相作用的的————可以用boosting或bagging算法。
	如果业务需求是要构建一个可以部署的模型，我们可以用回归或决策树模型（容易解释和说明），而不是黑盒算法如SVM，GBM等。


————————————————————————————————————————————————————————————————————————————————————————————————————
【标准化、归一化、中心化】
	归一化：
	1）把数据映射到0～1范围之内处理，数据处理更加便捷快速。
	2）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。
	3）归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。
	x' = (x - X_min) / (X_max - X_min)     Min-Max Normalization
	x' = (x - μ) / (MaxValue - MinValue)   平均归一化
	y = log10(x)                           对数函数转换
	y = atan(x) * 2 / π                    反余切函数转换
	
	标准化：
	1）标准化后每个特征的数值平均变为0、标准差变为1。
	2）标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。
  	x' = (x - μ)／σ
	
  	中心化：
	平均值为0，对标准差无要求。
	x' = x - μ
	
  	归一化和标准化的区别：
	归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1,1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
	标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。
	它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
  
    	标准化和中心化的区别：
	标准化是原始分数减去平均数然后除以标准差。
	中心化是原始分数减去平均数。 
	所以一般流程为先中心化再标准化。


 ————————————————————————————————————————————————————————————————————————————————————————————————————
  【为什么要进行数据标准化/归一化】
  	1）某些模型求解需要
	a 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，加快模型收敛速度。
	b 一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。
	  如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖。
	  
	2）无量纲化
  	例如房子数量和收入，因为从业务层知道，这两者的重要性一样，所以把它们全部归一化。 这是从业务层面上作的处理。

	3）避免数值问题
  	太大的数会引发数值问题。


————————————————————————————————————————————————————————————————————————————————————————————————————
【什么时候用归一化？什么时候用标准化？】
	1）如果对输出结果范围有要求，用归一化。
	2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
	3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
	
	在实际应用中，通过梯度下降法求解的模型一般都是需要归一化的，比如线性回归、logistic回归、KNN、SVM、神经网络等模型。
	但树形模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林。
	
	
————————————————————————————————————————————————————————————————————————————————————————————————————
【如何进行特征选择】
	1）Filter：过滤法，按照发散性或者相关性对各个特征进行行行评分，设定阈值或者待选择阈值的个数，选择特征。
	2）Wrapper：包装法，根据目目标函数(通常是预测效果评分)，每次选择若干干特征，或者排除若干干特征。
	3）Embedded：嵌入法，先使用用某些机器器学习的算法和模型进行行行训练，得到各个特征的权值系数，根据系数从大大到小小选择特征。
	类似于Filter方方法,但是是通过训练来确定特征的优劣。


————————————————————————————————————————————————————————————————————————————————————————————————————
【特征交叉(特征组合)方式有哪些】
	1）Dense特征组合
		将一个特征与其本身或其他特征相乘
		两个特征相除
		对连续特征进行分桶，以分为多个区间分箱
	2）ID特征之间的组合
		笛卡尔积：假如拥有一个特征A{A1,A2}。拥有一个特征B{B1,B2}
		A&B之间的交叉特征如下：{(A1,B1),(A1,B2),(A2,B1),(A2,B2)}
		经纬度


————————————————————————————————————————————————————————————————————————————————————————————————————
【请简要说说一个完整机器学习项目的流程】
	1）抽象成数学问题
		可以获得什么样的数据
		问题是 分类 or 回归 or 聚类

	2）获取数据
		数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
		数据要有代表性，否则必然会过拟合。
		对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
		对数据的量级有一个评估，多少个样本，多少个特征，判断内存是否够用。
		如果不够用，考虑改进算法或使用降维技巧。如果数据量实在太大，考虑分布式。

	3）特征预处理与特征选择
		特征预处理、数据清洗很关键。
		归一化、离散化、因子化、缺失值处理、去除共线性。
		筛选出显著特征、摒弃非显著特征，需要反复理解业务。
		特征有效性分析的相关技术：相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重。

	4）训练模型与调优
		调参需要对算法的原理有深入的理解。

	5）模型诊断
		过拟合、欠拟合判断：交叉验证，绘制学习曲线。
			过拟合：增加数据量，降低模型复杂度。
			欠拟合：提高特征数量和质量，增加模型复杂度。
		误差分析：是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题。
			诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，反复迭代不断逼近。

	6）模型融合
		一般模型融合后都能使得效果有一定提升。
		工程上，主要提升算法准确度的方法：1）模型的前端（特征清洗和预处理，不同的采样模式）2）后端（模型融合）。

	7）上线运行
		工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 
		1）准确程度、误差。2）其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。


————————————————————————————————————————————————————————————————————————————————————————————————————




————————————————————————————————————————————————————————————————————————————————————————————————————




————————————————————————————————————————————————————————————————————————————————————————————————————
