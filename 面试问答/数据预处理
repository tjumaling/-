————————————————————————————————————————————————————————————————————————————————————————————————————
【给定一个数据集，你如何决定使用哪一个算法】
	机器学习算法的选择完全取决于数据的类型。
	如果给定的一个数据集是线性的，线性回归是最好的选择。
	如果数据是图像或者音频，那么神经网络可以构建一个稳健的模型。
	如果该数据是非线性互相作用的的，可以用boosting或bagging算法。
	如果业务需求是要构建一个可以部署的模型，我们可以用回归或决策树模型（容易解释和说明），而不是黑盒算法如SVM，GBM等。
	总之，没有一个一劳永逸的算法。我们必须有足够的细心，去了解到底要用哪个算法。


————————————————————————————————————————————————————————————————————————————————————————————————————
【标准化、归一化、中心化】
	归一化：
	1）把数据映射到0～1范围之内处理，数据处理更加便捷快速。
	2）把有量纲表达式变成无量纲表达式，便于不同单位或量级的指标能够进行比较和加权。
	3）归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。
	x' = (x - X_min) / (X_max - X_min)     Min-Max Normalization
	x' = (x - μ) / (MaxValue - MinValue)   平均归一化
	y = log10(x)                           对数函数转换
	y = atan(x) * 2 / π                    反余切函数转换
	
	标准化：
	1）标准化后每个特征的数值平均变为0、标准差变为1。
	2）标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。
  	x' = (x - μ)／σ
	
  	中心化：
	平均值为0，对标准差无要求。
	x' = x - μ
	
  	归一化和标准化的区别：
	归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1,1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。
	标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。
	它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。
  
    	标准化和中心化的区别：
	标准化是原始分数减去平均数然后除以标准差。
	中心化是原始分数减去平均数。 
	所以一般流程为先中心化再标准化。


 ————————————————————————————————————————————————————————————————————————————————————————————————————
  【为什么要进行数据标准化/归一化】
  	1）某些模型求解需要
	a 在使用梯度下降的方法求解最优化问题时， 归一化/标准化后可以加快梯度下降的求解速度，加快模型收敛速度。
	b 一些分类器需要计算样本之间的距离(如欧氏距离)，例如KNN。
	  如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖。
	  
	2）无量纲化
  	例如房子数量和收入，因为从业务层知道，这两者的重要性一样，所以把它们全部归一化。 这是从业务层面上作的处理。

	3）避免数值问题
  	太大的数会引发数值问题。


————————————————————————————————————————————————————————————————————————————————————————————————————
【什么时候用归一化？什么时候用标准化？】
	1）如果对输出结果范围有要求，用归一化。
	2）如果数据较为稳定，不存在极端的最大最小值，用归一化。
	3）如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。
	
	在实际应用中，通过梯度下降法求解的模型一般都是需要归一化的，比如线性回归、logistic回归、KNN、SVM、神经网络等模型。
	但树形模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林。
	
	
————————————————————————————————————————————————————————————————————————————————————————————————————
【如何进行特征选择】
	1）Filter：过滤法，按照发散性或者相关性对各个特征进行行行评分，设定阈值或者待选择阈值的个数，选择特征。
	2）Wrapper：包装法，根据目目标函数(通常是预测效果评分)，每次选择若干干特征，或者排除若干干特征。
	3）Embedded：嵌入入法，先使用用某些机器器学习的算法和模型进行行行训练，得到各个特征的权值系数，根据系数从大大到小小选择特征。
	类似于Filter方方法,但是是通过训练来确定特征的优劣。









