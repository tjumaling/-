————————————————————————————————————————————————————————————————————————————————————————————————————————
【一句话介绍】
	逻辑斯蒂是二分类模型，假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数。

————————————————————————————————————————————————————————————————————————————————————————————————————————
【LR与线性回归的区别与联系】
	逻辑斯蒂回归和线性回归都是广义的线性回归。
	经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。
	LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围，其目标函数也因此从差平方和函数变为对数损失函数。
	以提供最优化所需导数（sigmoid函数是softmax函数的二元特例，其导数均为函数值的f*(1-f)形式）。
	LR往往是解决二元0/1分类问题的。
	多元分类，把sigmoid换成softmax。

————————————————————————————————————————————————————————————————————————————————————————————————————————
【LR和SVM的联系与区别】
	-----相同点-----
	1）都是线性分类器，本质上都是求一个最佳分类超平面。改进后可以处理多分类问题。
	2）都是监督学习。
	3）都是判别模型。
	-----不同点-----
	2）函数。LR：logistical loss，SVM：hinge loss。都增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 
	3）SVM只考虑support vectors，逻辑回归通过非线性映射，减小离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 
	4）LR模型更简单，好理解，适合处理大规模线性分类问题。
	   SVM的理解和优化相对来说复杂一些，转化为对偶问题后，分类只需要计算少数几个支持向量的距离，在进行复杂核函数计算时优势明显，可简化模型和计算。 

————————————————————————————————————————————————————————————————————————————————————————————————————————





————————————————————————————————————————————————————————————————————————————————————————————————————————






————————————————————————————————————————————————————————————————————————————————————————————————————————






————————————————————————————————————————————————————————————————————————————————————————————————————————
