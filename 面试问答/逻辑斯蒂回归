————————————————————————————————————————————————————————————————————————————————————————————————————————
【一句话介绍】
	逻辑斯蒂是二分类模型，假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数。

————————————————————————————————————————————————————————————————————————————————————————————————————————
【LR与线性回归的区别与联系】
	逻辑斯蒂回归和线性回归都是广义的线性回归。
	经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数。
	LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围，其目标函数也因此从差平方和函数变为对数损失函数。
	以提供最优化所需导数（sigmoid函数是softmax函数的二元特例，其导数均为函数值的f*(1-f)形式）。
	LR往往是解决二元0/1分类问题的。多元分类，把sigmoid换成softmax。

————————————————————————————————————————————————————————————————————————————————————————————————————————
【LR和SVM的联系与区别】
	-----相同点-----
	1）线性分类器，本质上都是求一个最佳分类超平面。改进后可处理多分类问题。
	2）监督学习。
	3）判别模型。
	-----不同点-----
	1）损失函数不同。LR：logistical loss（交叉熵），SVM：hinge loss。
	   LR基于概率理论，假设样本为正样本的概率可以用sigmoid函数表示，通过极大似然估计求参数的值。 
	   SVM基于几何间隔最大化，认为几何间隔最大的分类面为最优分类面。
	2）SVM只考虑support vectors，逻辑回归通过非线性映射，减小离分类平面远的点的权重。 
	   对数据和参数的敏感程度不同：SVM只考虑支持向量。其余地方添加或减少样本点对分类决策面没有任何影响； 
	   LR受所有数据点的影响。直接依赖数据分布，每个样本点都会影响决策面的结果。
	   如果训练数据不同类别严重不平衡，一般需要先做平衡处理，让不同类别的样本尽量平衡。
	3）SVM 基于距离分类，LR 基于概率分类。 SVM需要对数据先做normalization；LR不用。
	4）解决非线性问题时，支持向量机采用核函数，LR不采用核函数，计算量太大。 
	5）在小规模数据集上，Linear SVM略好于LR，但差别不大，且Linear SVM的计算复杂度受数据量限制，对海量数据LR使用更加广泛。
	6）SVM的损失函数就自带正则，而 LR 必须另外在损失函数之外添加正则项。 
	7）SVM的理解和优化更复杂，转化为对偶问题后，分类只需要计算少数几个支持向量的距离，在进行复杂核函数计算时优势明显，可简化模型和计算。 

————————————————————————————————————————————————————————————————————————————————————————————————————————





————————————————————————————————————————————————————————————————————————————————————————————————————————






————————————————————————————————————————————————————————————————————————————————————————————————————————






————————————————————————————————————————————————————————————————————————————————————————————————————————
