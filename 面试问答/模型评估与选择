【L1和L2的区别】
	L1范数: 为x向量各个元素绝对值之和。 
	L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或者Frobenius范数 
	Lp范数: 为x向量各个元素绝对值p次方和的1/p次方。
	
	L1范数可以使权值稀疏，方便特征提取。 
	L2范数可以防止过拟合，提升模型的泛化能力。
	
	在靠进零附近, L1以匀速下降到零, 而L2则完全停下来了。
	L1是将不重要的特征尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零。
	两者一起作用, 就是把重要性在一个数量级的那些特征一起平等共事。
	
	L1和L2正则先验分别服从什么分布，L1是拉普拉斯分布，L2是高斯分布。
	Lasso回归（L1）同时做变量选择和参数收缩，而ridge回归只做参数收缩，并最终在模型中包含所有的系数。
	
	为什么L1容易产生稀疏数据？正则函数的等高线和原始损失函数的等高线总是相交在坐标轴上。
	若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响
	但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。
	
————————————————————————————————————————————————————————————————————————————————————————————————————
【过拟合的原因，以及如何防止过拟合】
	过拟合是在训练集上的误差很小，而在测试集上的误差大，泛化能力差。
	1）选择特征
	2）正则化
	3）交叉叉验证提前终止
	4）加入随机性：随机森林随机选择特征；神经元以超参数p的概率被激活，Dropout（随机删除神经元）
	5）增大训练量：数据源头，重采样，根据分布产生数据，复制数据加入噪声（图像旋转，裁剪，模糊，调亮度，平移，镜像）
	
————————————————————————————————————————————————————————————————————————————————————————————————————
【共线性和过拟合
